{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NativeTensor:\n",
    "    \n",
    "    def __init__(self, values):\n",
    "        self.backing = values\n",
    "        \n",
    "    def from_values(values):\n",
    "        return NativeTensor(values)\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.backing.size\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.backing.shape\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return NativeTensor(self.backing[index])\n",
    "\n",
    "    def reveal(self):\n",
    "        return self\n",
    "    \n",
    "    def unwrap(self):\n",
    "        return self.backing\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"NativeTensor(%s)\" % self.backing\n",
    "    \n",
    "    def wrap_if_needed(y):\n",
    "        if isinstance(y, int) or isinstance(y, float):\n",
    "            return NativeTensor.from_values(np.array([y]))\n",
    "        if isinstance(y, np.ndarray):\n",
    "            return NativeTensor.from_values(y)\n",
    "        return y\n",
    "    \n",
    "    def add(x, y):\n",
    "        y = NativeTensor.wrap_if_needed(y)\n",
    "        \n",
    "        if isinstance(y, NativeTensor):\n",
    "            return NativeTensor(x.backing + y.backing)\n",
    "        \n",
    "        if isinstance(y, PublicEncodedTensor):\n",
    "            return PublicEncodedTensor.from_values(x).add(y)\n",
    "        \n",
    "        if isinstance(y, SharedEncodedTensor):\n",
    "            return PublicEncodedTensor.from_values(x).add(y)\n",
    "        \n",
    "        raise TypeError(\"%s does not support %s\" % (type(x), type(y)))\n",
    "        \n",
    "    def __add__(x, y):\n",
    "        return x.add(y)\n",
    "    \n",
    "    def sub(x, y):\n",
    "        y = NativeTensor.wrap_if_needed(y)\n",
    "        \n",
    "        if isinstance(y, NativeTensor):\n",
    "            return NativeTensor(x.backing - y.backing)\n",
    "        \n",
    "        if isinstance(y, PublicEncodedTensor):\n",
    "            return PublicEncodedTensor.from_values(x).sub(y)\n",
    "        \n",
    "        if isinstance(y, SharedEncodedTensor):\n",
    "            return PublicEncodedTensor.from_values(x).sub(y)\n",
    "        \n",
    "        raise TypeError(\"%s does not support %s\" % (type(x), type(y)))\n",
    "        \n",
    "    def __sub__(x, y):\n",
    "        return x.sub(y)\n",
    "    \n",
    "    def mul(x, y):\n",
    "        y = NativeTensor.wrap_if_needed(y)\n",
    "        \n",
    "        if isinstance(y, NativeTensor):\n",
    "            return NativeTensor(x.backing * y.backing)\n",
    "        \n",
    "        if isinstance(y, PublicEncodedTensor):\n",
    "            return PublicEncodedTensor.from_values(x).mul(y)\n",
    "        \n",
    "        if isinstance(y, SharedEncodedTensor):\n",
    "            return PublicEncodedTensor.from_values(x).mul(y)\n",
    "        \n",
    "        raise TypeError(\"%s does not support %s\" % (type(x), type(y)))\n",
    "        \n",
    "    def __mul__(x, y):\n",
    "        return x.mul(y)\n",
    "        \n",
    "    def dot(x, y):\n",
    "        y = NativeTensor.wrap_if_needed(y)\n",
    "        \n",
    "        if isinstance(y, NativeTensor):\n",
    "            return NativeTensor(x.backing.dot(y.backing))\n",
    "        \n",
    "        if isinstance(y, PublicEncodedTensor):\n",
    "            return PublicEncodedTensor.from_values(x).dot(y)\n",
    "        \n",
    "        if isinstance(y, SharedEncodedTensor):\n",
    "            return PublicEncodedTensor.from_values(x).dot(y)\n",
    "        \n",
    "        raise TypeError(\"%s does not support %s\" % (type(x), type(y)))\n",
    "        \n",
    "    def div(x, y):\n",
    "        y = NativeTensor.wrap_if_needed(y)\n",
    "        \n",
    "        if isinstance(y, NativeTensor):\n",
    "            return NativeTensor(x.backing / y.backing)\n",
    "        \n",
    "        raise TypeError(\"%s does not support %s\" % (type(x), type(y)))\n",
    "        \n",
    "    def __div__(x, y):\n",
    "        return x.div(y)\n",
    "        \n",
    "    def transpose(x):\n",
    "        return NativeTensor(x.backing.T)\n",
    "    \n",
    "    def neg(x):\n",
    "        return NativeTensor(0 - x.backing)\n",
    "\n",
    "    def sum0(x):\n",
    "        return NativeTensor(x.backing.sum(axis=0, keepdims=True))\n",
    "    \n",
    "    def sum1(x):\n",
    "        return NativeTensor(x.backing.sum(axis=1, keepdims=True))\n",
    "    \n",
    "    def exp(x):\n",
    "        return NativeTensor(np.exp(x.backing))\n",
    "    \n",
    "    def log(x):\n",
    "        return NativeTensor(np.log(x.backing))\n",
    "    \n",
    "    def inv(x):\n",
    "        return NativeTensor(1. / x.backing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = NativeTensor(np.array([1,2,3,4]).reshape(4,1) - 2)\n",
    "# y = NativeTensor(np.array([5,6,7,8]).reshape(4,1))\n",
    "# z = x + y; print(z)\n",
    "# z = (x * y); print(z)\n",
    "# z = x.dot(y.transpose()); print(z)\n",
    "# z = z.sum0(); print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Field Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = 'object'\n",
    "Q = 2657003489534545107915232808830590043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FieldTensor:\n",
    "    \n",
    "#     def __init__(self, elements):\n",
    "#         if not isinstance(elements, np.ndarray):\n",
    "#             elements = np.array([elements])\n",
    "#         self.elements = elements\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return \"FieldTensor(%s)\" % self.elements\n",
    "        \n",
    "#     @property\n",
    "#     def shape(self):\n",
    "#         return self.elements.shape\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         return FieldTensor(self.elements[index])\n",
    "    \n",
    "#     def add(x, y):\n",
    "#         if isinstance(y, FieldTensor):\n",
    "#             return FieldTensor((x.elements + y.elements) % Q)\n",
    "#         else:\n",
    "#             raise TypeError(type(y))\n",
    "        \n",
    "#     def __add__(x, y):\n",
    "#         return x.add(y)\n",
    "        \n",
    "#     def sub(x, y):\n",
    "#         if isinstance(y, FieldTensor):\n",
    "#             return FieldTensor((x.elements - y.elements) % Q)\n",
    "#         else:\n",
    "#             raise TypeError(type(y))\n",
    "        \n",
    "#     def __sub__(x, y):\n",
    "#         return x.sub(y)\n",
    "        \n",
    "#     def mul(x, y):\n",
    "#         if isinstance(y, FieldTensor):\n",
    "#             return FieldTensor((x.elements * y.elements) % Q)\n",
    "#         else:\n",
    "#             raise TypeError(type(y))\n",
    "    \n",
    "#     def __mul__(x, y):\n",
    "#         return x.mul(y)\n",
    "        \n",
    "#     def dot(x, y):\n",
    "#         if isinstance(y, FieldTensor):\n",
    "#             return FieldTensor(x.elements.dot(y.elements) % Q)\n",
    "#         else:\n",
    "#             raise TypeError(type(y))\n",
    "        \n",
    "#     def transpose(x):\n",
    "#         return FieldTensor(x.elements.T)\n",
    "    \n",
    "#     def sum0(x):\n",
    "#         return FieldTensor(x.elements.sum(axis=0, keepdims=True))\n",
    "    \n",
    "#     def sum1(x):\n",
    "#         return FieldTensor(x.elements.sum(axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PublicFieldTensor:\n",
    "    \n",
    "    def __init__(self, elements):\n",
    "        self.elements = elements\n",
    "\n",
    "    def from_elements(elements):\n",
    "        return PublicFieldTensor(elements)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"PublicFieldTensor(%s)\" % self.elements\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.elements.size\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.elements.shape\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return PublicFieldTensor.from_elements(self.elements[index])\n",
    "    \n",
    "    def add(x, y):\n",
    "        if isinstance(y, PublicFieldTensor):\n",
    "            elements = (x.elements + y.elements) % Q\n",
    "            return PublicFieldTensor.from_elements(elements)\n",
    "        \n",
    "        elif isinstance(y, SharedFieldTensor):\n",
    "            shares0 = (x.elements + y.shares0) % Q\n",
    "            shares1 =               y.shares1\n",
    "            return SharedFieldTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(type(y))\n",
    "        \n",
    "    def __add__(x, y):\n",
    "        return x.add(y)\n",
    "    \n",
    "    def mul(x, y):\n",
    "        if isinstance(y, PublicFieldTensor):\n",
    "            elements = (x.elements * y.elements) % Q\n",
    "            return PublicFieldTensor.from_elements(elements)\n",
    "        \n",
    "        if isinstance(y, SharedFieldTensor):\n",
    "            shares0 = (x.elements * y.shares0) % Q\n",
    "            shares1 = (x.elements * y.shares1) % Q\n",
    "            return SharedFieldTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(type(y))\n",
    "    \n",
    "    def __mul__(x, y):\n",
    "        return x.mul(y)\n",
    "    \n",
    "    def dot(x, y):\n",
    "        if isinstance(y, PublicFieldTensor):\n",
    "            elements = (x.elements.dot(y.elements)) % Q\n",
    "            return PublicFieldTensor.from_elements(elements)\n",
    "        \n",
    "        if isinstance(y, SharedFieldTensor):\n",
    "            shares0 = (x.elements.dot(y.shares0)) % Q\n",
    "            shares1 = (x.elements.dot(y.shares1)) % Q\n",
    "            return SharedFieldTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def share(elements):\n",
    "    shares0 = np.array([ random.randrange(Q) for _ in range(elements.size) ]).astype(DTYPE).reshape(elements.shape)\n",
    "    shares1 = (elements - shares0) % Q\n",
    "    return shares0, shares1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedFieldTensor:\n",
    "    \n",
    "    def __init__(self, elements, shares0=[], shares1=[]):\n",
    "        if elements is not None:\n",
    "            shares0, shares1 = share(elements)\n",
    "        self.shares0 = shares0\n",
    "        self.shares1 = shares1\n",
    "        assert self.shares0.shape == self.shares1.shape\n",
    "    \n",
    "    def from_elements(elements):\n",
    "        return SharedFieldTensor(elements)\n",
    "    \n",
    "    def from_shares(shares0, shares1):\n",
    "        return SharedFieldTensor(None, shares0, shares1)\n",
    "    \n",
    "    def reveal(self):\n",
    "        elements = (self.shares0 + self.shares1) % Q\n",
    "        return PublicFieldTensor.from_elements(elements)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SharedFieldTensor(%s)\" % self.reveal().elements\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return SharedFieldTensor.from_shares(self.shares0[index], self.shares1[index])\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.shares0.size\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.shares0.shape\n",
    "    \n",
    "    def add(x, y):\n",
    "        if isinstance(y, SharedFieldTensor):\n",
    "            shares0 = (x.shares0 + y.shares0) % Q\n",
    "            shares1 = (x.shares1 + y.shares1) % Q\n",
    "            return SharedFieldTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        elif isinstance(y, PublicFieldTensor):\n",
    "            shares0 = (x.shares0 + y.elements) % Q\n",
    "            shares1 =  x.shares1\n",
    "            return SharedFieldTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(type(y))\n",
    "        \n",
    "    def __add__(x, y):\n",
    "        return x.add(y)\n",
    "    \n",
    "    def mul(x, y):\n",
    "        if isinstance(y, PublicFieldTensor):\n",
    "            shares0 = (x.shares0 * y.elements) % Q\n",
    "            shares1 = (x.shares1 * y.elements) % Q\n",
    "            return SharedFieldTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(type(y))\n",
    "    \n",
    "    def __mul__(x, y):\n",
    "        return x.mul(y)\n",
    "    \n",
    "    def dot(x, y):\n",
    "        if isinstance(y, PublicFieldTensor):\n",
    "            shares0 = (x.shares0.dot(y.elements)) % Q\n",
    "            shares1 = (x.shares1.dot(y.elements)) % Q\n",
    "            return SharedFieldTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoded Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "log2 = lambda x: log(x)/log(2)\n",
    "\n",
    "# for arbitrary precision ints\n",
    "\n",
    "# we need room for summing MAX_SUM values of MAX_DEGREE before during modulus reduction\n",
    "MAX_DEGREE = 2\n",
    "MAX_SUM = 2**12\n",
    "assert MAX_DEGREE * log2(Q) + log2(MAX_SUM) < 256\n",
    "\n",
    "BASE = 2\n",
    "PRECISION_INTEGRAL   = 16\n",
    "PRECISION_FRACTIONAL = 32\n",
    "# TODO Gap as needed for local truncating\n",
    "\n",
    "# we need room for double precision before truncating\n",
    "assert PRECISION_INTEGRAL + 2 * PRECISION_FRACTIONAL < log(Q)/log(BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(rationals):\n",
    "    return (rationals * BASE**PRECISION_FRACTIONAL).astype('int').astype(DTYPE) % Q\n",
    "\n",
    "def decode(elements):\n",
    "    map_negative_range = np.vectorize(lambda element: element if element <= Q/2 else element - Q)\n",
    "    return map_negative_range(elements) / BASE**PRECISION_FRACTIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PublicEncodedTensor:\n",
    "    \n",
    "    def __init__(self, values, elements=None):\n",
    "        if not values is None:\n",
    "            if not isinstance(values, np.ndarray):\n",
    "                values = np.array([values])\n",
    "            elements = encode(values)\n",
    "        assert isinstance(elements, np.ndarray), \"%s, %s, %s\" % (values, elements, type(elements))\n",
    "        self.elements = elements\n",
    "    \n",
    "    def from_values(values):\n",
    "        return PublicEncodedTensor(values)\n",
    "    \n",
    "    def from_elements(elements):\n",
    "        return PublicEncodedTensor(None, elements)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return PublicEncodedTensor.from_elements(self.elements[index])\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.elements.shape\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.elements.size\n",
    "    \n",
    "    def unwrap(self):\n",
    "        return decode(self.elements)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"PublicEncodedTensor(%s)\" % decode(self.elements)\n",
    "    \n",
    "    def reveal(self):\n",
    "        return NativeTensor.from_values(decode(self.elements))\n",
    "    \n",
    "    def truncate(self, amount=PRECISION_FRACTIONAL):\n",
    "        positive_numbers = (self.elements <= Q // 2).astype(int)\n",
    "        elements = self.elements\n",
    "        elements = (Q + (2 * positive_numbers - 1) * elements) % Q # x if x <= Q//2 else Q - x\n",
    "        elements = np.floor_divide(elements, BASE**amount)         # x // BASE**amount\n",
    "        elements = (Q + (2 * positive_numbers - 1) * elements) % Q # x if x <= Q//2 else Q - x\n",
    "        return PublicEncodedTensor.from_elements(elements)\n",
    "    \n",
    "    def wrap_if_needed(y):\n",
    "        if isinstance(y, int) or isinstance(y, float):\n",
    "            return PublicEncodedTensor.from_values(np.array([y]))\n",
    "        if isinstance(y, np.ndarray):\n",
    "            return PublicEncodedTensor.from_values(y)\n",
    "        if isinstance(y, NativeTensor):\n",
    "            return PublicEncodedTensor.from_values(y.backing)\n",
    "        return y\n",
    "    \n",
    "    def add(x, y):\n",
    "        y = PublicEncodedTensor.wrap_if_needed(y)\n",
    "        \n",
    "        if isinstance(y, PublicEncodedTensor):\n",
    "            return PublicEncodedTensor.from_elements((x.elements + y.elements) % Q)\n",
    "        \n",
    "        if isinstance(y, SharedEncodedTensor):\n",
    "            shares0 = (x.elements + y.shares0) % Q\n",
    "            shares1 =               y.shares1\n",
    "            return SharedEncodedTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        raise TypeError(\"%s does not support %s\" % (type(x), type(y)))\n",
    "        \n",
    "    def __add__(x, y):\n",
    "        return x.add(y)\n",
    "        \n",
    "    def sub(x, y):\n",
    "        y = PublicEncodedTensor.wrap_if_needed(y)\n",
    "        \n",
    "        if isinstance(y, PublicEncodedTensor):\n",
    "            return PublicEncodedTensor.from_elements((x.elements - y.elements) % Q)\n",
    "        \n",
    "        if isinstance(y, SharedEncodedTensor):\n",
    "            return x.add(y.neg()) # TODO there might be a more efficient way\n",
    "            \n",
    "        raise TypeError(\"%s does not support %s\" % (type(x), type(y)))\n",
    "        \n",
    "    def __sub__(x, y):\n",
    "        return x.sub(y)\n",
    "        \n",
    "    def mul(x, y):\n",
    "        y = PublicEncodedTensor.wrap_if_needed(y)\n",
    "        \n",
    "        if isinstance(y, PublicEncodedTensor):\n",
    "            return PublicEncodedTensor.from_elements((x.elements * y.elements) % Q).truncate()\n",
    "        \n",
    "        if isinstance(y, PublicFieldTensor):\n",
    "            return PublicFieldTensor.from_elements((x.elements * y.elements) % Q)\n",
    "    \n",
    "        raise TypeError(\"%s does not support %s\" % (type(x), type(y)))\n",
    "    \n",
    "    def __mul__(x, y):\n",
    "        return x.mul(y)\n",
    "    \n",
    "    def dot(x, y):\n",
    "        y = PublicEncodedTensor.wrap_if_needed(y)\n",
    "        \n",
    "        if isinstance(y, PublicEncodedTensor):\n",
    "            return PublicEncodedTensor.from_elements(x.elements.dot(y.elements) % Q).truncate()\n",
    "\n",
    "        raise TypeError(\"%s does not support %s\" % (type(x), type(y)))\n",
    "        \n",
    "    def transpose(x):\n",
    "        return PublicEncodedTensor.from_elements(x.elements.T)\n",
    "    \n",
    "    def sum0(x):\n",
    "        return PublicEncodedTensor.from_elements(x.elements.sum(axis=0, keepdims=True))\n",
    "    \n",
    "    def sum1(x):\n",
    "        return PublicEncodedTensor.from_elements(x.elements.sum(axis=1, keepdims=True))\n",
    "    \n",
    "    def neg(self):\n",
    "        minus_one = PublicFieldTensor.from_elements(np.array([Q - 1]))\n",
    "        z = self.mul(minus_one)\n",
    "        return PublicEncodedTensor.from_elements(z.elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PublicEncodedTensor([[-1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 2.]])\n",
      "PublicEncodedTensor([[ 5.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 8.]])\n",
      "PublicEncodedTensor([[  4.]\n",
      " [  6.]\n",
      " [  8.]\n",
      " [ 10.]])\n",
      "PublicEncodedTensor([[ -5.]\n",
      " [  0.]\n",
      " [  7.]\n",
      " [ 16.]])\n",
      "PublicEncodedTensor([[ -5.  -6.  -7.  -8.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  5.   6.   7.   8.]\n",
      " [ 10.  12.  14.  16.]])\n",
      "PublicEncodedTensor([[ 10.  12.  14.  16.]])\n"
     ]
    }
   ],
   "source": [
    "# x = PublicEncodedTensor(np.array([1,2,3,4]).reshape(4,1) - 2); print(x)\n",
    "# y = PublicEncodedTensor(np.array([5,6,7,8]).reshape(4,1)); print(y)\n",
    "# z = x + y; print(z)\n",
    "# z = (x * y); print(z)\n",
    "# z = x.dot(y.transpose()); print(z)\n",
    "# z = z.sum0(); print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncodedTensor:\n",
    "    \n",
    "#     def __init__(self, values, elements=None):\n",
    "#         if not elements is None:\n",
    "#             assert isinstance(elements, FieldTensor)\n",
    "#             self.elements = elements\n",
    "#         else:\n",
    "#             if not isinstance(values, np.ndarray): values = np.array([values])\n",
    "#             self.elements = FieldTensor(encode(values))\n",
    "    \n",
    "#     @property\n",
    "#     def shape(self):\n",
    "#         return self.elements.shape\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return \"EncodedTensor(%s)\" % decode(self.elements)\n",
    "    \n",
    "#     def with_elements(elements):\n",
    "#         return EncodedTensor(None, elements)\n",
    "    \n",
    "#     def truncate(self, amount=PRECISION_FRACTIONAL):        \n",
    "#         truncate = np.vectorize(lambda x: x // BASE**amount if x <= Q/2 else Q - ((Q - x) // BASE**amount))\n",
    "#         return EncodedTensor.with_elements(truncate(self.elements))\n",
    "    \n",
    "#     def add(x, y):\n",
    "#         if isinstance(y, EncodedTensor):\n",
    "#             return EncodedTensor.with_elements((x.elements + y.elements) % Q)\n",
    "#         elif isinstance(y, PrivateTensor):\n",
    "#             shares0 = (x.elements + y.shares0) % Q\n",
    "#             shares1 =               y.shares1\n",
    "#             return PrivateTensor.with_shares(shares0, shares1)\n",
    "#         else:\n",
    "#             raise TypeError(type(y))\n",
    "        \n",
    "#     def __add__(x, y):\n",
    "#         return x.add(y)\n",
    "        \n",
    "#     def sub(x, y):\n",
    "#         if isinstance(y, EncodedTensor):\n",
    "#             return EncodedTensor.with_elements((x.elements - y.elements) % Q)\n",
    "#         elif isinstance(y, PrivateTensor):\n",
    "#             # TODO might be more efficient way\n",
    "#             shares0 = ((Q-1) * y.shares0 + x.elements) % Q\n",
    "#             shares1 = ((Q-1) * y.shares1) % Q\n",
    "#             return PrivateTensor.with_shares(shares0, shares1)\n",
    "#         else:\n",
    "#             raise TypeError(type(y))\n",
    "        \n",
    "#     def __sub__(x, y):\n",
    "#         return x.sub(y)\n",
    "        \n",
    "#     def mul(x, y, truncate=True):\n",
    "#         if isinstance(y, EncodedTensor):\n",
    "#             z = EncodedTensor.with_elements((x.elements * y.elements) % Q)\n",
    "#             if truncate: z = z.truncate()\n",
    "#             return z\n",
    "#         elif isinstance(y, PrivateTensor):\n",
    "#             shares0 = (x.elements * y.shares0) % Q\n",
    "#             shares1 = (x.elements * y.shares1) % Q\n",
    "#             z = PrivateTensor.with_shares(shares0, shares1)\n",
    "#             if truncate: z = z.truncate()\n",
    "#             return z\n",
    "#         else:\n",
    "#             raise TypeError(type(y))\n",
    "    \n",
    "#     def __mul__(x, y):\n",
    "#         return x.mul(y)\n",
    "    \n",
    "#     def div(x, y):\n",
    "#         if isinstance(y, EncodedTensor):\n",
    "#             return EncodedTensor(decode(x.elements) / decode(y.elements))\n",
    "#         elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.ndarray):\n",
    "#             return EncodedTensor(decode(x.elements) / y)\n",
    "#         else:\n",
    "#             raise TypeError(type(y))\n",
    "        \n",
    "#     def __div__(x, y):\n",
    "#         return x.div(y)\n",
    "    \n",
    "#     def dot(x, y, truncate=True):\n",
    "#         if isinstance(y, EncodedTensor):\n",
    "#             z = EncodedTensor.with_elements(x.elements.dot(y.elements) % Q)\n",
    "#             if truncate: z = z.truncate()\n",
    "#             return z\n",
    "#         elif isinstance(y, PrivateTensor):\n",
    "#             shares0 = x.elements.dot(y.shares0) % Q\n",
    "#             shares1 = x.elements.dot(y.shares1) % Q\n",
    "#             z = PrivateTensor.with_shares(shares0, shares1)\n",
    "#             if truncate: z = z.truncate()\n",
    "#             return z\n",
    "#         else:\n",
    "#             raise TypeError(type(y))\n",
    "        \n",
    "#     def transpose(x):\n",
    "#         return EncodedTensor.with_elements(x.elements.T)\n",
    "    \n",
    "#     def sum0(x):\n",
    "#         return EncodedTensor.with_elements(x.elements.sum(axis=0, keepdims=True))\n",
    "    \n",
    "#     def sum1(x):\n",
    "#         return EncodedTensor.with_elements(x.elements.sum(axis=1, keepdims=True))\n",
    "        \n",
    "#     def exp(x):\n",
    "#         return EncodedTensor(np.exp(decode(x.elements)))\n",
    "    \n",
    "#     def __getitem__(x, index):\n",
    "#         return EncodedTensor.with_elements(x.elements[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = EncodedTensor(np.array([1,2,3,4]).reshape(4,1) - 2); print(x)\n",
    "# y = EncodedTensor(np.array([5,6,7,8]).reshape(4,1)); print(y)\n",
    "# z = x + y; print(z)\n",
    "# z = (x * y); print(z)\n",
    "# z = x.dot(y.transpose()); print(z)\n",
    "# z = z.sum0(); print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mul_triple(shape):\n",
    "    a = np.array([ random.randrange(Q) for _ in range(np.prod(shape)) ]).astype(DTYPE).reshape(shape)\n",
    "    b = np.array([ random.randrange(Q) for _ in range(np.prod(shape)) ]).astype(DTYPE).reshape(shape)\n",
    "    ab = (a * b) % Q\n",
    "    return SharedFieldTensor.from_elements(a), \\\n",
    "           SharedFieldTensor.from_elements(b), \\\n",
    "           SharedFieldTensor.from_elements(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dot_triple(m, n, o):\n",
    "    a = np.array([ random.randrange(Q) for _ in range(m*n) ]).astype(DTYPE).reshape((m,n))\n",
    "    b = np.array([ random.randrange(Q) for _ in range(n*o) ]).astype(DTYPE).reshape((n,o))\n",
    "    ab = np.dot(a, b)\n",
    "    return SharedFieldTensor.from_elements(a), \\\n",
    "           SharedFieldTensor.from_elements(b), \\\n",
    "           SharedFieldTensor.from_elements(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEncodedTensor:\n",
    "    \n",
    "    def __init__(self, values, shares0=[], shares1=[]):\n",
    "        if values is not None:\n",
    "            if not isinstance(values, np.ndarray):\n",
    "                values = np.array(values)\n",
    "            shares0, shares1 = share(encode(values))\n",
    "        self.shares0 = shares0\n",
    "        self.shares1 = shares1\n",
    "        assert self.shares0.shape == self.shares1.shape\n",
    "    \n",
    "    def from_values(values):\n",
    "        return SharedEncodedTensor(values)\n",
    "    \n",
    "    def from_elements(elements):\n",
    "        shares0, shares1 = share(elements)\n",
    "        return SharedEncodedTensor(None, shares0, shares1)\n",
    "    \n",
    "    def from_shares(shares0, shares1):\n",
    "        return SharedEncodedTensor(None, shares0, shares1)\n",
    "    \n",
    "    def unwrap(self):\n",
    "        return decode((self.shares0 + self.shares1) % Q)\n",
    "    \n",
    "#     def reveal(self):\n",
    "#         return PublicEncodedTensor.from_elements((self.shares0 + self.shares1) % Q)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        elements = (self.shares0 + self.shares1) % Q\n",
    "        return \"SharedEncodedTensor(%s)\" % decode(elements)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return SharedEncodedTensor.from_shares(self.shares0[index], self.shares1[index])\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.shares0.shape\n",
    "    \n",
    "    def truncate(self, amount=PRECISION_FRACTIONAL):\n",
    "        shares0 = np.floor_divide(self.shares0, BASE**amount) % Q\n",
    "        shares1 = (Q - (np.floor_divide(Q - self.shares1, BASE**amount))) % Q\n",
    "        return SharedEncodedTensor.from_shares(shares0, shares1)\n",
    "    \n",
    "    def add(x, y):\n",
    "        if isinstance(y, SharedEncodedTensor):\n",
    "            shares0 = (x.shares0 + y.shares0) % Q\n",
    "            shares1 = (x.shares1 + y.shares1) % Q\n",
    "            return SharedEncodedTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        elif isinstance(y, PublicEncodedTensor):\n",
    "            shares0 = (x.shares0 + y.elements) % Q\n",
    "            shares1 =  x.shares1\n",
    "            return SharedEncodedTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.ndarray):\n",
    "            return x.add(PublicEncodedTensor.from_values(y))\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(type(y))\n",
    "        \n",
    "    def __add__(x, y):\n",
    "        return x.add(y)\n",
    "    \n",
    "    def sub(x, y):\n",
    "        if isinstance(y, SharedEncodedTensor):\n",
    "            shares0 = (x.shares0 - y.shares0) % Q\n",
    "            shares1 = (x.shares1 - y.shares1) % Q\n",
    "            return SharedEncodedTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        elif isinstance(y, PublicEncodedTensor):\n",
    "            shares0 = (x.shares0 - y.elements) % Q\n",
    "            shares1 =  x.shares1\n",
    "            return SharedEncodedTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.ndarray):\n",
    "            return x.sub(PublicEncodedTensor.from_values(y))\n",
    "        \n",
    "        elif isinstance(y, SharedFieldTensor):\n",
    "            shares0 = (x.shares0 - y.shares0) % Q\n",
    "            shares1 = (x.shares1 - y.shares1) % Q\n",
    "            return SharedFieldTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(type(y))\n",
    "        \n",
    "    def __sub__(x, y):\n",
    "        return x.sub(y)\n",
    "    \n",
    "    def mul(x, y, precomputed=None):\n",
    "        if isinstance(y, SharedEncodedTensor):\n",
    "            if precomputed is None: precomputed = generate_mul_triple(x.shape)\n",
    "            a, b, ab = precomputed\n",
    "            assert x.shape == y.shape\n",
    "            assert x.shape == a.shape\n",
    "            assert y.shape == b.shape\n",
    "            alpha = (x - a).reveal()\n",
    "            beta  = (y - b).reveal()\n",
    "            z = alpha.mul(beta) + \\\n",
    "                alpha.mul(b) + \\\n",
    "                a.mul(beta) + \\\n",
    "                ab\n",
    "            return SharedEncodedTensor.from_shares(z.shares0, z.shares1).truncate()\n",
    "        \n",
    "        elif isinstance(y, PublicEncodedTensor):\n",
    "            shares0 = (x.shares0 * y.elements) % Q\n",
    "            shares1 = (x.shares1 * y.elements) % Q\n",
    "            return SharedEncodedTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.ndarray):\n",
    "            return x.mul(PublicEncodedTensor.from_values(y))\n",
    "        \n",
    "        elif isinstance(y, SharedFieldTensor):\n",
    "            shares0 = (x.shares0 * y.shares0) % Q\n",
    "            shares1 = (x.shares1 * y.shares1) % Q\n",
    "            return SharedFieldTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        elif isinstance(y, PublicFieldTensor):\n",
    "            shares0 = (x.shares0 * y.elements) % Q\n",
    "            shares1 = (x.shares1 * y.elements) % Q\n",
    "            return SharedFieldTensor.from_shares(shares0, shares1)\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(type(y))\n",
    "        \n",
    "    def __mul__(x, y):\n",
    "        return x.mul(y)\n",
    "    \n",
    "    def dot(x, y, precomputed=None):\n",
    "        if isinstance(y, SharedEncodedTensor):\n",
    "            m = x.shape[0]\n",
    "            n = x.shape[1]\n",
    "            o = y.shape[1]\n",
    "            assert n == y.shape[0]\n",
    "            if precomputed is None: precomputed = generate_dot_triple(m, n, o)\n",
    "            a, b, ab = precomputed\n",
    "            alpha = (x - a).reveal()\n",
    "            beta  = (y - b).reveal()\n",
    "            z = alpha.dot(beta) + \\\n",
    "                alpha.dot(b) + \\\n",
    "                a.dot(beta) + \\\n",
    "                ab\n",
    "            return SharedEncodedTensor.from_shares(z.shares0, z.shares1).truncate()\n",
    "        \n",
    "        elif isinstance(y, PublicEncodedTensor):\n",
    "            assert x.shape[1] == y.shape[0]\n",
    "            shares0 = x.shares0.dot(y.elements) % Q\n",
    "            shares1 = x.shares1.dot(y.elements) % Q\n",
    "            return SharedEncodedTensor.from_shares(shares0, shares1).truncate()\n",
    "        \n",
    "        elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.ndarray):\n",
    "            return x.dot(PublicEncodedTensor.from_values(y))\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(type(y))\n",
    "        \n",
    "    def neg(self):\n",
    "        minus_one = PublicFieldTensor.from_elements(np.array([Q - 1]))\n",
    "        z = self.mul(minus_one)\n",
    "        return SharedEncodedTensor.from_shares(z.shares0, z.shares1)\n",
    "        \n",
    "    def transpose(self):\n",
    "        return SharedEncodedTensor.from_shares(self.shares0.T, self.shares1.T)\n",
    "    \n",
    "    def sum0(self):\n",
    "        shares0 = self.shares0.sum(axis=0, keepdims=True) % Q\n",
    "        shares1 = self.shares1.sum(axis=0, keepdims=True) % Q\n",
    "        return SharedEncodedTensor.from_shares(shares0, shares1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SharedEncodedTensor([[  4.]\n",
      " [  6.]\n",
      " [  8.]\n",
      " [ 10.]])\n",
      "SharedEncodedTensor([[ -5.]\n",
      " [  0.]\n",
      " [  7.]\n",
      " [ 16.]])\n",
      "SharedEncodedTensor([[ -5.  -6.  -7.  -8.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  5.   6.   7.   8.]\n",
      " [ 10.  12.  14.  16.]])\n",
      "SharedEncodedTensor([[ 10.  12.  14.  16.]])\n"
     ]
    }
   ],
   "source": [
    "x = SharedEncodedTensor(np.array([1,2,3,4]).reshape(4,1) - 2)\n",
    "y = SharedEncodedTensor(np.array([5,6,7,8]).reshape(4,1))\n",
    "z = x + y; print(z)\n",
    "z = (x * y); print(z)\n",
    "z = x.dot(y.transpose()); print(z)\n",
    "z = z.sum0(); print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PrivateEncodedTensor:\n",
    "    \n",
    "#     def __init__(self, values, apply_encoding=True, shares0=None, shares1=None):\n",
    "#         if shares0 is not None and shares1 is not None:\n",
    "#             self.shares0 = shares0\n",
    "#             self.shares1 = shares1\n",
    "#         else:\n",
    "#             if apply_encoding: values = encode(values)\n",
    "#             self.shares0 = np.array([ random.randrange(Q) for _ in range(values.size) ]).astype(DTYPE).reshape(values.shape)\n",
    "#             self.shares1 = (values - self.shares0) % Q\n",
    "        \n",
    "#     def with_shares(shares0, shares1):\n",
    "#         return PrivateTensor(None, False, shares0, shares1)\n",
    "    \n",
    "#     def reveal(self):\n",
    "#         values = (self.shares0 + self.shares1) % Q\n",
    "#         return EncodedTensor.with_elements(values)\n",
    "    \n",
    "#     def truncate(self, amount=PRECISION_FRACTIONAL):\n",
    "#         # TODO truncation doesn't work if there are 0 values -- assert this?\n",
    "#         shares0 = np.floor_divide(self.shares0, BASE**amount) % Q\n",
    "#         shares1 = (Q - (np.floor_divide(Q - self.shares1, BASE**amount))) % Q\n",
    "#         return PrivateTensor.with_shares(shares0, shares1)\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return \"PrivateTensor(%s)\" % decode(self.reveal().elements)\n",
    "    \n",
    "#     @property\n",
    "#     def shape(self):\n",
    "#         return self.shares0.shape\n",
    "    \n",
    "#     def add(x, y):\n",
    "#         if isinstance(y, PrivateEncodedTensor):\n",
    "#             shares0 = (x.shares0 + y.shares0) % Q\n",
    "#             shares1 = (x.shares1 + y.shares1) % Q\n",
    "#             return PrivateEncodedTensor.with_shares(shares0, shares1)\n",
    "#         elif isinstance(y, EncodedTensor):\n",
    "#             shares0 = (x.shares0 + y.elements) % Q\n",
    "#             shares1 =  x.shares1\n",
    "#             return PrivateTensor.with_shares(shares0, shares1)\n",
    "#         elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.ndarray):\n",
    "#             return x.add(EncodedTensor(y))\n",
    "#         else:\n",
    "#             raise TypeError(type(y))\n",
    "        \n",
    "#     def __add__(x, y):\n",
    "#         return x.add(y)\n",
    "    \n",
    "#     def sub(x, y):\n",
    "#         if isinstance(y, PrivateTensor):\n",
    "#             shares0 = (x.shares0 - y.shares0) % Q\n",
    "#             shares1 = (x.shares1 - y.shares1) % Q\n",
    "#             return PrivateTensor.with_shares(shares0, shares1)\n",
    "#         elif isinstance(y, EncodedTensor):\n",
    "#             shares0 = (x.shares0 - y.values) % Q\n",
    "#             shares1 =  x.shares1\n",
    "#             return PrivateTensor.with_shares(shares0, shares1)\n",
    "#         elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.ndarray):\n",
    "#             return x.sub(EncodedTensor(y))\n",
    "#         else:\n",
    "#             raise TypeError(type(y))\n",
    "        \n",
    "#     def __sub__(x, y):\n",
    "#         return x.sub(y)\n",
    "    \n",
    "#     def mul(x, y, truncate=True, precomputed=None):\n",
    "#         if isinstance(y, PrivateTensor):\n",
    "#             assert x.shares0.shape == x.shares1.shape == y.shares0.shape == y.shares1.shape\n",
    "#             if precomputed is None: precomputed = generate_mul_triple(x.shares0.shape)\n",
    "#             a, b, ab = precomputed\n",
    "#             assert x.shape == a.shape\n",
    "#             assert y.shape == b.shape\n",
    "#             alpha = (x - a).reveal()\n",
    "#             beta  = (y - b).reveal()\n",
    "#             z = alpha.mul(beta, truncate=False) + \\\n",
    "#                 alpha.mul(b, truncate=False) + \\\n",
    "#                 a.mul(beta, truncate=False) + \\\n",
    "#                 ab\n",
    "#             if truncate: z = z.truncate()\n",
    "#             return z\n",
    "#         elif isinstance(y, EncodedTensor):\n",
    "#             shares0 = (x.shares0 * y.values) % Q\n",
    "#             shares1 = (x.shares1 * y.values) % Q\n",
    "#             z = PrivateTensor.with_shares(shares0, shares1)\n",
    "#             if truncate: z = z.truncate()\n",
    "#             return z\n",
    "#         elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.ndarray):\n",
    "#             return x.mul(EncodedTensor(y), truncate, precomputed)\n",
    "#         else:\n",
    "#             raise TypeError(type(y))\n",
    "        \n",
    "#     def __mul__(x, y):\n",
    "#         return x.mul(y)\n",
    "        \n",
    "#     def dot(x, y, truncate=True, precomputed=None):\n",
    "#         if isinstance(y, PrivateTensor):\n",
    "#             m = x.shares0.shape[0]\n",
    "#             n = x.shares0.shape[1]\n",
    "#             o = y.shares0.shape[1]\n",
    "#             assert n == y.shares0.shape[0]\n",
    "#             if precomputed is None: precomputed = generate_dot_triple(m, n, o)\n",
    "#             a, b, ab = precomputed\n",
    "#             alpha = (x - a).reveal()\n",
    "#             beta  = (y - b).reveal()\n",
    "#             z = alpha.dot(beta, truncate=False) + \\\n",
    "#                 alpha.dot(b, truncate=False) + \\\n",
    "#                 a.dot(beta, truncate=False) + \\\n",
    "#                 ab\n",
    "#             if truncate: z = z.truncate()\n",
    "#             return z\n",
    "#         elif isinstance(y, EncodedTensor):\n",
    "#             assert x.shares0.shape == x.shares1.shape\n",
    "#             assert x.shares0.shape[0] == y.shape[1]\n",
    "#             shares0 = x.shares0.dot(y.values) % Q\n",
    "#             shares1 = x.shares1.dot(y.values) % Q\n",
    "#             z = PrivateTensor.with_shares(shares0, shares1)\n",
    "#             if truncate: z = z.truncate()\n",
    "#             return z\n",
    "#         elif isinstance(y, int) or isinstance(y, float) or isinstance(y, np.ndarray):\n",
    "#             return x.dot(EncodedTensor(y))\n",
    "#         else:\n",
    "#             raise TypeError(type(y))\n",
    "        \n",
    "#     def neg(self):\n",
    "#         return self.mul(EncodedTensor.with_elements(np.array([Q - 1])), truncate=False)\n",
    "        \n",
    "#     def transpose(self):\n",
    "#         return PrivateTensor.with_shares(self.shares0.T, self.shares1.T)\n",
    "    \n",
    "#     def sum0(self):\n",
    "#         shares0 = self.shares0.sum(axis=0, keepdims=True) % Q\n",
    "#         shares1 = self.shares1.sum(axis=0, keepdims=True) % Q\n",
    "#         return PrivateTensor.with_shares(shares0, shares1)\n",
    "    \n",
    "#     def __getitem__(x, index):\n",
    "#         return PrivateTensor.with_shares(x.shares0[index], x.shares1[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = PrivateTensor(np.array([1,2,3,4]).reshape(4,1) - 2)\n",
    "# y = PrivateTensor(np.array([5,6,7,8]).reshape(4,1))\n",
    "# z = x + y; print(z)\n",
    "# z = (x * y); print(z)\n",
    "# z = x.dot(y.transpose()); print(z)\n",
    "# z = z.sum0(); print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(10)\n",
    "isinstance(x, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([784087], dtype=object)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(np.array([0.0001825597]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
